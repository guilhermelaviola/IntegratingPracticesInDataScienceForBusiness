{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeTFmeN3YRP2QLgMIaMxjJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilhermelaviola/IntegratingPracticesInDataScienceForBusiness/blob/main/Class15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization and Count Vectorization**\n",
        "Tokenization is the process of dividing a text into smaller units called tokens, which can be words, subwords, characters, or even sentences. This is essential because analyzing raw text without any kind of division is ineffective. An organized framework is needed to apply machine learning algorithms and other data analysis techniques."
      ],
      "metadata": {
        "id": "H4zetrE41L9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd9KnpGCzP6t",
        "outputId": "33ed40b1-9953-4002-d113-a38a8bfb3c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Importing all the necessary libraries:\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download punkt_tab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a text for analysis and applying word and sentence tokenization:\n",
        "text = 'Tokenization is a fundamental step in natural language processing. It is essential for sentiment analysis.'\n",
        "\n",
        "tokenized_words = word_tokenize(text)\n",
        "tokenized_sentences = sent_tokenize(text)\n",
        "\n",
        "print('Original text', text)\n",
        "print('Tokenized words', tokenized_words)\n",
        "print('Tokenized senteces', tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCu8k3PEzfqv",
        "outputId": "1f763783-24df-4562-c233-65dd6d3a3d5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text Tokenization is a fundamental step in natural language processing. It is essential for sentiment analysis.\n",
            "Tokenized words ['Tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'is', 'essential', 'for', 'sentiment', 'analysis', '.']\n",
            "Tokenized senteces ['Tokenization is a fundamental step in natural language processing.', 'It is essential for sentiment analysis.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a list of documents (texts) and transforming them into numeric vectors based on word frequency:\n",
        "documents = [\n",
        "    'Tokenization is a fundamental step in natural language processing.',\n",
        "    'It is essential for sentiment analysis and other data processing.'\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "print('Vocabulary words:', vectorizer.get_feature_names_out())\n",
        "print('Vector representation of documents:\\n', X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpiLtTeV0U7a",
        "outputId": "4741894b-a67c-4f4e-d7cf-2310a5332de3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary words: ['analysis' 'and' 'data' 'essential' 'for' 'fundamental' 'in' 'is' 'it'\n",
            " 'language' 'natural' 'other' 'processing' 'sentiment' 'step'\n",
            " 'tokenization']\n",
            "Vector representation of documents:\n",
            " [[0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1]\n",
            " [1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0]]\n"
          ]
        }
      ]
    }
  ]
}